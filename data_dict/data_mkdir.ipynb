{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dataset dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import h5py\n",
    "import xmltodict\n",
    "import glob\n",
    "import yaml\n",
    "import tqdm\n",
    "import pickle\n",
    "import random\n",
    "from os import listdir\n",
    "from os.path import isfile, join, splitext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Skyra'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf = h5py.File('/media/hdd1/fastMRIdata/brain/multicoil_train/file_brain_AXFLAIR_200_6002429.h5')\n",
    "dic = xmltodict.parse(hf['ismrmrd_header'][()])['ismrmrdHeader']\n",
    "dic['acquisitionSystemInformation']['systemModel']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf['kspace'][()].shape[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "brain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_protocol = 'AXT1POST'   # AXT1, AXT2, AXFLAIR, AXT1POST, CORPD_FBK\n",
    "target_system = 'Avanto'      # Aera, Skyra\n",
    "\n",
    "dataset_name = 'brain_train_' + target_protocol + '_' + target_system + '_11-12'\n",
    "\n",
    "mypath = '/media/hdd1/fastMRIdata/brain/multicoil_train/'\n",
    "# '/media/hdd1/fastMRIdata/brain/multicoil_train/'\n",
    "# '/media/hdd1/fastMRIdata/brain/multicoil_val/'\n",
    "\n",
    "\n",
    "# read file name in \"mypath\" into list\n",
    "filenames = [splitext(filename)[0] for filename in listdir(mypath)]\n",
    "slice_list = []\n",
    "\n",
    "for filename in filenames:\n",
    "    file_name = mypath + filename + '.h5'\n",
    "    hf = h5py.File(file_name)\n",
    "    dic = xmltodict.parse(hf['ismrmrd_header'][()])['ismrmrdHeader']\n",
    "    reconX = int(dic['encoding']['reconSpace']['matrixSize']['x'])\n",
    "    reconY = int(dic['encoding']['reconSpace']['matrixSize']['y'])\n",
    "    kspace_shape = hf['kspace'].shape\n",
    "    protocolName = dict(hf.attrs)['acquisition']\n",
    "    systemModel = dic['acquisitionSystemInformation']['systemModel']\n",
    "    if  protocolName == target_protocol and systemModel == target_system\\\n",
    "        and reconX == 320 and reconY == 320: \n",
    "        # slice_list.append({\n",
    "        #     'path': mypath + filename+'.h5', \n",
    "        #     'slice': 0, \n",
    "        #     'filename': filename, \n",
    "        #     'predefined_mask': None})   \n",
    "        # slice_list.append({\n",
    "        #     'path': mypath + filename+'.h5', \n",
    "        #     'slice': 1, \n",
    "        #     'filename': filename, \n",
    "        #     'predefined_mask': None})      \n",
    "        slice_list.append({\n",
    "            'path': mypath + filename+'.h5', \n",
    "            'slice': 10, \n",
    "            'filename': filename, \n",
    "            'predefined_mask': None})        \n",
    "        slice_list.append({\n",
    "            'path': mypath + filename+'.h5', \n",
    "            'slice': 11, \n",
    "            'filename': filename, \n",
    "            'predefined_mask': None})\n",
    "\n",
    "\n",
    "    with open(\"/cheng/metaMRI/metaMRI/data_dict_temp/\" + dataset_name + \".yaml\", \"w\") as fp:\n",
    "        yaml.dump(slice_list,fp)\n",
    "        \n",
    "    if len(slice_list) == 500:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_protocol = 'CORPDFS_FBK'    # AXT1, AXT2, AXFLAIR, AXT1POST, CORPD_FBK\n",
    "target_system = 'Biograph_mMR'      # Aera, Skyra, 'Biograph_mMR'\n",
    "\n",
    "dataset_name = 'knee_val_' + target_protocol + '_' + target_system + '_10-17'\n",
    "\n",
    "mypath = '/media/hdd1/fastMRIdata/knee/multicoil_val/'\n",
    "# '/srv/knee/multicoil_train/'\n",
    "# '/media/hdd1/fastMRIdata/knee/multicoil_val/'\n",
    "\n",
    "# read file name in \"mypath\" into list\n",
    "filenames = [splitext(filename)[0] for filename in listdir(mypath)]\n",
    "slice_list = []\n",
    "\n",
    "for filename in filenames:\n",
    "    file_name = mypath + filename + '.h5'\n",
    "    hf = h5py.File(file_name)\n",
    "    dic = xmltodict.parse(hf['ismrmrd_header'][()])['ismrmrdHeader']\n",
    "    reconX = int(dic['encoding']['reconSpace']['matrixSize']['x'])\n",
    "    reconY = int(dic['encoding']['reconSpace']['matrixSize']['y'])\n",
    "    kspace_shape = hf['kspace'].shape\n",
    "    protocolName = dict(hf.attrs)['acquisition']\n",
    "    systemModel = dic['acquisitionSystemInformation']['systemModel']\n",
    "    if  protocolName == target_protocol and systemModel == target_system : \n",
    "        slice_list.append({\n",
    "            'path': mypath + filename+'.h5', \n",
    "            'slice': 9, \n",
    "            'filename': filename, \n",
    "            'predefined_mask': None})  \n",
    "        slice_list.append({\n",
    "            'path': mypath + filename+'.h5', \n",
    "            'slice': 10, \n",
    "            'filename': filename, \n",
    "            'predefined_mask': None})      \n",
    "        slice_list.append({\n",
    "            'path': mypath + filename+'.h5', \n",
    "            'slice': 11, \n",
    "            'filename': filename, \n",
    "            'predefined_mask': None})   \n",
    "        slice_list.append({\n",
    "            'path': mypath + filename+'.h5', \n",
    "            'slice': 12, \n",
    "            'filename': filename, \n",
    "            'predefined_mask': None})   \n",
    "        slice_list.append({\n",
    "            'path': mypath + filename+'.h5', \n",
    "            'slice': 13, \n",
    "            'filename': filename, \n",
    "            'predefined_mask': None})   \n",
    "        slice_list.append({\n",
    "            'path': mypath + filename+'.h5', \n",
    "            'slice': 14, \n",
    "            'filename': filename, \n",
    "            'predefined_mask': None})        \n",
    "        slice_list.append({\n",
    "            'path': mypath + filename+'.h5', \n",
    "            'slice': 15, \n",
    "            'filename': filename, \n",
    "            'predefined_mask': None})        \n",
    "        slice_list.append({\n",
    "            'path': mypath + filename+'.h5', \n",
    "            'slice': 16, \n",
    "            'filename': filename, \n",
    "            'predefined_mask': None})\n",
    "\n",
    "    \n",
    "    with open(\"/cheng/metaMRI/metaMRI/data_dict_temp/\" + dataset_name + \".yaml\", \"w\") as fp:\n",
    "        yaml.dump(slice_list,fp)\n",
    "    \n",
    "    if len(slice_list) == 500:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TTT paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filename:  file1000292\n",
      "slice:  17\n",
      "filename:  file1000254\n",
      "slice:  16\n",
      "filename:  file1000178\n",
      "slice:  16\n",
      "filename:  file1000026\n",
      "slice:  15\n",
      "filename:  file1000229\n",
      "slice:  17\n",
      "filename:  file1000247\n",
      "slice:  18\n",
      "filename:  file1000017\n",
      "slice:  17\n",
      "filename:  file1000206\n",
      "slice:  17\n",
      "filename:  file1000114\n",
      "slice:  19\n",
      "filename:  file1000631\n",
      "slice:  20\n",
      "filename:  file1000480\n",
      "slice:  16\n",
      "filename:  file1000555\n",
      "slice:  15\n",
      "filename:  file1001159\n",
      "slice:  15\n",
      "filename:  file1000810\n",
      "slice:  18\n",
      "filename:  file1000201\n",
      "slice:  20\n",
      "filename:  file1001059\n",
      "slice:  15\n",
      "filename:  file1000647\n",
      "slice:  18\n",
      "filename:  file1000344\n",
      "slice:  15\n",
      "filename:  file1000325\n",
      "slice:  16\n",
      "filename:  file1001143\n",
      "slice:  16\n",
      "filename:  file1000748\n",
      "slice:  16\n",
      "filename:  file1001825\n",
      "slice:  16\n",
      "filename:  file1001984\n",
      "slice:  16\n",
      "filename:  file1001031\n",
      "slice:  19\n",
      "filename:  file1001450\n",
      "slice:  17\n",
      "filename:  file1001275\n",
      "slice:  17\n",
      "filename:  file1001140\n",
      "slice:  20\n",
      "filename:  file1001480\n",
      "slice:  15\n",
      "filename:  file1001703\n",
      "slice:  18\n",
      "filename:  file1000932\n",
      "slice:  15\n",
      "filename:  file1001818\n",
      "slice:  17\n",
      "filename:  file1001959\n",
      "slice:  18\n",
      "filename:  file1002538\n",
      "slice:  17\n",
      "filename:  file1001916\n",
      "slice:  17\n",
      "filename:  file1002155\n",
      "slice:  17\n",
      "filename:  file1001188\n",
      "slice:  17\n",
      "filename:  file1002515\n",
      "slice:  21\n",
      "filename:  file1001997\n",
      "slice:  17\n",
      "filename:  file1002404\n",
      "slice:  16\n",
      "filename:  file1001499\n",
      "slice:  15\n",
      "filename:  file1002412\n",
      "slice:  17\n",
      "filename:  file1001557\n",
      "slice:  15\n",
      "filename:  file1001798\n",
      "slice:  17\n",
      "filename:  file1001289\n",
      "slice:  20\n",
      "filename:  file1002380\n",
      "slice:  15\n",
      "filename:  file1000942\n",
      "slice:  17\n",
      "filename:  file1000858\n",
      "slice:  16\n",
      "filename:  file1001566\n",
      "slice:  20\n",
      "filename:  file1001533\n",
      "slice:  17\n",
      "filename:  file1001119\n",
      "slice:  18\n",
      "filename:  file1002002\n",
      "slice:  17\n",
      "filename:  file1000291\n",
      "slice:  16\n",
      "filename:  file1002214\n",
      "slice:  18\n",
      "filename:  file1000007\n",
      "slice:  19\n",
      "filename:  file1000591\n",
      "slice:  19\n",
      "filename:  file1001938\n",
      "slice:  22\n",
      "filename:  file1000538\n",
      "slice:  20\n",
      "filename:  file1000196\n",
      "slice:  18\n",
      "filename:  file1000280\n",
      "slice:  16\n",
      "filename:  file1000528\n",
      "slice:  19\n",
      "filename:  file1002145\n",
      "slice:  19\n",
      "filename:  file1000000\n",
      "slice:  17\n",
      "filename:  file1001262\n",
      "slice:  18\n",
      "filename:  file1001955\n",
      "slice:  17\n",
      "filename:  file1000273\n",
      "slice:  17\n",
      "filename:  file1001191\n",
      "slice:  16\n",
      "filename:  file1001344\n",
      "slice:  21\n",
      "filename:  file1002377\n",
      "slice:  19\n",
      "filename:  file1000593\n",
      "slice:  18\n",
      "filename:  file1001643\n",
      "slice:  21\n",
      "filename:  file1001850\n",
      "slice:  19\n",
      "filename:  file1001440\n",
      "slice:  16\n",
      "filename:  file1001851\n",
      "slice:  20\n",
      "filename:  file1000990\n",
      "slice:  20\n",
      "filename:  file1000903\n",
      "slice:  16\n",
      "filename:  file1001862\n",
      "slice:  18\n",
      "filename:  file1002007\n",
      "slice:  19\n",
      "filename:  file1000818\n",
      "slice:  18\n",
      "filename:  file1001655\n",
      "slice:  18\n",
      "filename:  file1001144\n",
      "slice:  17\n",
      "filename:  file1001429\n",
      "slice:  21\n",
      "filename:  file1001968\n",
      "slice:  19\n",
      "filename:  file1000972\n",
      "slice:  21\n",
      "filename:  file1001338\n",
      "slice:  18\n",
      "filename:  file1001834\n",
      "slice:  20\n",
      "filename:  file1001689\n",
      "slice:  17\n",
      "filename:  file1002340\n",
      "slice:  20\n",
      "filename:  file1002067\n",
      "slice:  18\n",
      "filename:  file1002451\n",
      "slice:  23\n",
      "filename:  file1001651\n",
      "slice:  20\n",
      "filename:  file1001365\n",
      "slice:  17\n",
      "filename:  file1001090\n",
      "slice:  19\n",
      "filename:  file1002436\n",
      "slice:  18\n",
      "filename:  file1002159\n",
      "slice:  16\n",
      "filename:  file1000476\n",
      "slice:  20\n",
      "filename:  file1001726\n",
      "slice:  16\n",
      "filename:  file1002389\n",
      "slice:  19\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "with open('/cheng/metaMRI/ttt_for_deep_learning_cs/unet/train_data/fs_val','rb') as fn:\n",
    "    dataset = pickle.load(fn)\n",
    "\n",
    "mypath = '/fastMRIdata/knee/multicoil_val/'\n",
    "slice_list = []\n",
    "\n",
    "for i,d in enumerate(dataset):\n",
    "    file_name = d['filename']\n",
    "    filename, file_extension = os.path.splitext(file_name)\n",
    "    print(\"filename: \", filename)\n",
    "    slice = d['slice']\n",
    "    print(\"slice: \", slice)\n",
    "    slice_list.append({\n",
    "            'path': mypath + filename+'.h5', \n",
    "            'slice': slice, \n",
    "            'filename': filename, \n",
    "            'predefined_mask': None})  \n",
    "    \n",
    "with open(\"/cheng/metaMRI/metaMRI/data_dict_temp/\" + \"fs_val.yaml\", \"w\") as fp:\n",
    "    yaml.dump(slice_list,fp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
