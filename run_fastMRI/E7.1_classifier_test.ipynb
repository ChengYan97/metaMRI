{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fe2d8c22930>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'\n",
    "import torch\n",
    "import learn2learn as l2l\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "# The corase reconstruction is the rss of the zerofilled multi-coil kspaces\n",
    "# after inverse FT.\n",
    "from functions.data.transforms import UnetDataTransform, UnetDataTransform_norm, normalize\n",
    "# Import a torch.utils.data.Dataset class that takes a list of data examples, a path to those examples\n",
    "# a data transform and outputs a torch dataset.\n",
    "from functions.data.mri_dataset import SliceDataset\n",
    "# Unet architecture as nn.Module\n",
    "from functions.models.unet import Unet\n",
    "# Function that returns a MaskFunc object either for generatig random or equispaced masks\n",
    "from functions.data.subsample import create_mask_for_mask_type\n",
    "# Implementation of SSIMLoss\n",
    "from functions.training.losses import SSIMLoss\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "SEED = 1\n",
    "# seed\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.manual_seed(SEED)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target data1 shape:  torch.Size([150, 102400])\n",
      "Target data2 shape:  torch.Size([150, 102400])\n"
     ]
    }
   ],
   "source": [
    "path_data1 = '/cheng/metaMRI/metaMRI/data_dict/E7.1/brain_train_FLAIR_Skyra_5-8.yaml'\n",
    "path_data2 = '/cheng/metaMRI/metaMRI/data_dict/E7.1/brain_train_FLAIR_Prisma_5-8.yaml'\n",
    "\n",
    "\n",
    "# hyperparameter\n",
    "target_shot = 150\n",
    "\n",
    "# mask function and data transform\n",
    "mask_function = create_mask_for_mask_type(mask_type_str = 'random', self_sup = False, \n",
    "                    center_fraction = 0.08, acceleration = 4.0, acceleration_total = 3.0)\n",
    "\n",
    "# fix mask\n",
    "data_transform = UnetDataTransform_norm('multicoil', mask_func = mask_function, use_seed=True, mode='adapt')\n",
    "\n",
    "target_set1 = SliceDataset(dataset = path_data1, path_to_dataset='', path_to_sensmaps=None, provide_senmaps=False, \n",
    "                        challenge=\"multicoil\", transform=data_transform, use_dataset_cache=True, num_samples = target_shot)\n",
    "target_set2 = SliceDataset(dataset = path_data2, path_to_dataset='', path_to_sensmaps=None, provide_senmaps=False, \n",
    "                        challenge=\"multicoil\", transform=data_transform, use_dataset_cache=True, num_samples = target_shot)\n",
    "\n",
    "dataloader1 = torch.utils.data.DataLoader(target_set1, batch_size = 1, num_workers = 8, \n",
    "                        generator = torch.Generator().manual_seed(1), pin_memory = True)\n",
    "dataloader2 = torch.utils.data.DataLoader(target_set2, batch_size = 1, num_workers = 8, \n",
    "                        generator = torch.Generator().manual_seed(1), pin_memory = True)\n",
    "\n",
    "\n",
    "flattened_target_set1 = []\n",
    "for images in dataloader1:\n",
    "    input_image, target_image, mean, std, fname, slice_num = images\n",
    "    flattened_input = torch.flatten(input_image, start_dim=1, end_dim=3)\n",
    "    flattened_target_set1.extend(flattened_input)\n",
    "flattened_target_set2 = []\n",
    "for images in dataloader2:\n",
    "    input_image, target_image, mean, std, fname, slice_num = images\n",
    "    flattened_input = torch.flatten(input_image, start_dim=1, end_dim=3)\n",
    "    flattened_target_set2.extend(flattened_input)\n",
    "\n",
    "\n",
    "flattened_target_set1 = torch.stack(flattened_target_set1)\n",
    "flattened_target_set2 = torch.stack(flattened_target_set2)\n",
    "print('Target data1 shape: ', flattened_target_set1.shape)\n",
    "print('Target data2 shape: ', flattened_target_set2.shape)\n",
    "\n",
    "# convert to numpy\n",
    "target_1 = flattened_target_set1.numpy()\n",
    "target_2 = flattened_target_set2.numpy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = [0] * target_shot\n",
    "for i in range(target_shot+1, target_shot*2+1):\n",
    "    Y.append(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9034733"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA \n",
    "# concat the data\n",
    "flattened_target_set = np.concatenate((flattened_target_set1, flattened_target_set2), axis=0)\n",
    "# dimension reduction evaluation\n",
    "N = 100\n",
    "pca = PCA(n_components=N).fit(flattened_target_set) \n",
    "pca_ratio = pca.explained_variance_ratio_\n",
    "np.sum(pca_ratio[0:N-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 100)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_pca = PCA(n_components=N).fit_transform(flattened_target_set) \n",
    "target_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,\n",
       "       1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "       1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,\n",
       "       1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,\n",
       "       1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,\n",
       "       1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,\n",
       "       0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,\n",
       "       1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0,\n",
       "       0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,\n",
       "       1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1,\n",
       "       0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1], dtype=int32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=2, random_state=0).fit(target_pca)\n",
    "kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching ratio: 53.00%\n"
     ]
    }
   ],
   "source": [
    "matching_elements = np.equal(kmeans.labels_, Y)\n",
    "matching_ratio = np.sum(matching_elements) / matching_elements.size\n",
    "print(\"Matching ratio: {:.2f}%\".format(matching_ratio * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "target_tsne = np.concatenate((flattened_target_set1, flattened_target_set2), axis=0)\n",
    "X_tsne = TSNE(n_components=3, learning_rate='auto', init='random', perplexity=30).fit_transform(target_tsne)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,\n",
       "       0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "       1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans = KMeans(n_clusters=2, random_state=0).fit(X_tsne)\n",
    "kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching ratio: 35.33%\n"
     ]
    }
   ],
   "source": [
    "matching_elements = np.equal(kmeans.labels_, Y)\n",
    "matching_ratio = np.sum(matching_elements) / matching_elements.size\n",
    "print(\"Matching ratio: {:.2f}%\".format(matching_ratio * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
